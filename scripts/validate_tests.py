#!/usr/bin/env python3
"""
æµ‹è¯•éªŒè¯è„šæœ¬ï¼šè‡ªåŠ¨è¿è¡Œæ‰€æœ‰æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š

åŠŸèƒ½:
1. è¿è¡Œå…¨éƒ¨æµ‹è¯•å¥—ä»¶
2. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
3. éªŒè¯æµ‹è¯•ç»“æœ
4. æ£€æŸ¥ä»£ç è¦†ç›–ç‡
5. ç”Ÿæˆæ€§èƒ½åˆ†æ

ä½¿ç”¨æ–¹æ³•:
    python scripts/validate_tests.py [options]
    
å‚æ•°:
    --fast: å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡æ…¢é€Ÿæµ‹è¯•
    --coverage: ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
    --report: ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    --fix: è‡ªåŠ¨ä¿®å¤å¯ä¿®å¤çš„é—®é¢˜
"""

import os
import sys
import json
import time
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class TestValidator:
    """æµ‹è¯•éªŒè¯å™¨"""
    
    def __init__(self, fast_mode: bool = False, with_coverage: bool = False, 
                 generate_report: bool = True, auto_fix: bool = False):
        self.project_root = project_root
        self.fast_mode = fast_mode
        self.with_coverage = with_coverage
        self.generate_report = generate_report
        self.auto_fix = auto_fix
        
        self.reports_dir = project_root / 'tests' / 'reports'
        self.reports_dir.mkdir(exist_ok=True)
        
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'project_info': self._get_project_info(),
            'configuration': {
                'fast_mode': fast_mode,
                'with_coverage': with_coverage,
                'auto_fix': auto_fix
            },
            'test_results': {},
            'summary': {
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 0,
                'error_tests': 0,
                'skipped_tests': 0,
                'execution_time': 0,
                'success_rate': 0
            },
            'issues': [],
            'recommendations': []
        }
    
    def run_validation(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•éªŒè¯"""
        print("\nğŸš€ å¼€å§‹æµ‹è¯•éªŒè¯...")
        start_time = time.time()
        
        success = True
        
        try:
            # 1. ç¯å¢ƒæ£€æŸ¥
            print("\n1ï¸âƒ£ æ£€æŸ¥ç¯å¢ƒ...")
            if not self._check_environment():
                success = False
            
            # 2. è¿è¡Œå•å…ƒæµ‹è¯•
            print("\n2ï¸âƒ£ è¿è¡Œå•å…ƒæµ‹è¯•...")
            unit_result = self._run_unit_tests()
            self.validation_results['test_results']['unit_tests'] = unit_result
            if not unit_result['success']:
                success = False
            
            # 3. è¿è¡Œé›†æˆæµ‹è¯•
            print("\n3ï¸âƒ£ è¿è¡Œé›†æˆæµ‹è¯•...")
            integration_result = self._run_integration_tests()
            self.validation_results['test_results']['integration_tests'] = integration_result
            if not integration_result['success']:
                success = False
            
            # 4. è¿è¡Œæ•°æ®éªŒè¯æµ‹è¯•
            print("\n4ï¸âƒ£ è¿è¡Œæ•°æ®éªŒè¯æµ‹è¯•...")
            data_validation_result = self._run_data_validation_tests()
            self.validation_results['test_results']['data_validation'] = data_validation_result
            if not data_validation_result['success']:
                success = False
            
            # 5. è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼ˆå¦‚æœä¸æ˜¯å¿«é€Ÿæ¨¡å¼ï¼‰
            if not self.fast_mode:
                print("\n5ï¸âƒ£ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
                performance_result = self._run_performance_tests()
                self.validation_results['test_results']['performance'] = performance_result
                if not performance_result['success']:
                    success = False
            
            # 6. ä»£ç è´¨é‡æ£€æŸ¥
            print("\n6ï¸âƒ£ æ£€æŸ¥ä»£ç è´¨é‡...")
            quality_result = self._check_code_quality()
            self.validation_results['test_results']['code_quality'] = quality_result
            if not quality_result['success'] and not self.auto_fix:
                success = False
            
            # 7. ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Šï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.with_coverage:
                print("\n7ï¸âƒ£ ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š...")
                coverage_result = self._generate_coverage_report()
                self.validation_results['test_results']['coverage'] = coverage_result
            
            # 8. ç»Ÿè®¡ç»“æœ
            self._calculate_summary()
            
        except Exception as e:
            print(f"\nâ— éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            self.validation_results['issues'].append(f"éªŒè¯è¿‡ç¨‹é”™è¯¯: {e}")
            success = False
        
        # è®°å½•æ‰§è¡Œæ—¶é—´
        self.validation_results['summary']['execution_time'] = time.time() - start_time
        
        # 9. ç”ŸæˆæŠ¥å‘Š
        if self.generate_report:
            print("\n8ï¸âƒ£ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
            self._generate_final_report()
        
        # 10. æ˜¾ç¤ºç»“æœ
        self._display_summary(success)
        
        return success
    
    def _check_environment(self) -> bool:
        """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
        success = True
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        python_version = sys.version_info
        if python_version < (3, 9):
            self.validation_results['issues'].append(f"Pythonç‰ˆæœ¬è¿‡ä½: {python_version}, éœ€è¦ >= 3.9")
            success = False
        
        # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
        required_files = [
            'pyproject.toml',
            'src/__init__.py',
            'tests/__init__.py',
            'tests/conftest.py'
        ]
        
        for file_path in required_files:
            if not (self.project_root / file_path).exists():
                self.validation_results['issues'].append(f"ç¼ºå¤±å¿…è¦æ–‡ä»¶: {file_path}")
                success = False
        
        # æ£€æŸ¥ä¾èµ–
        try:
            import pytest
            import pandas
            import numpy
            print("  âœ… æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
        except ImportError as e:
            self.validation_results['issues'].append(f"ç¼ºå¤±ä¾èµ–: {e}")
            success = False
        
        return success
    
    def _run_unit_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        cmd = ['python', '-m', 'pytest', 'tests/unit/', '-v', '--tb=short']
        
        if self.fast_mode:
            cmd.extend(['-x'])  # é‡åˆ°ç¬¬ä¸€ä¸ªå¤±è´¥å°±åœæ­¢
        
        result = self._run_command(cmd)
        stats = self._parse_pytest_output(result['stdout'])
        
        return {
            'command': ' '.join(cmd),
            'returncode': result['returncode'],
            'stats': stats,
            'duration': result.get('duration', 0),
            'success': result['returncode'] == 0 and stats['failed'] == 0,
            'output': result['stdout'][-1000:] if len(result['stdout']) > 1000 else result['stdout']
        }
    
    def _run_integration_tests(self) -> Dict[str, Any]:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        cmd = ['python', '-m', 'pytest', 'tests/integration/', '-v', '--tb=short']
        
        if self.fast_mode:
            cmd.extend(['-m', 'not slow'])
        
        result = self._run_command(cmd)
        stats = self._parse_pytest_output(result['stdout'])
        
        return {
            'command': ' '.join(cmd),
            'returncode': result['returncode'],
            'stats': stats,
            'duration': result.get('duration', 0),
            'success': result['returncode'] == 0 and stats['failed'] == 0,
            'output': result['stdout'][-1000:] if len(result['stdout']) > 1000 else result['stdout']
        }
    
    def _run_data_validation_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ•°æ®éªŒè¯æµ‹è¯•"""
        cmd = ['python', '-m', 'pytest', 'tests/data_validation/', '-v', '--tb=short']
        
        result = self._run_command(cmd)
        stats = self._parse_pytest_output(result['stdout'])
        
        return {
            'command': ' '.join(cmd),
            'returncode': result['returncode'],
            'stats': stats,
            'duration': result.get('duration', 0),
            'success': result['returncode'] == 0 and stats['failed'] == 0,
            'output': result['stdout'][-1000:] if len(result['stdout']) > 1000 else result['stdout']
        }
    
    def _run_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        cmd = ['python', '-m', 'pytest', 'tests/performance/', '-v', '--tb=short', '--durations=10']
        
        result = self._run_command(cmd)
        stats = self._parse_pytest_output(result['stdout'])
        
        return {
            'command': ' '.join(cmd),
            'returncode': result['returncode'],
            'stats': stats,
            'duration': result.get('duration', 0),
            'success': result['returncode'] == 0 and stats['failed'] == 0,
            'output': result['stdout'][-1000:] if len(result['stdout']) > 1000 else result['stdout']
        }
    
    def _check_code_quality(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç è´¨é‡"""
        issues = []
        checks_performed = []
        
        # æ£€æŸ¥flake8
        if self._has_command('flake8'):
            flake8_result = self._run_command(['flake8', 'src/', 'tests/', '--max-line-length=100'])
            checks_performed.append('flake8')
            if flake8_result['returncode'] != 0:
                issues.append(f"Flake8ä»£ç è´¨é‡é—®é¢˜")
                if self.auto_fix:
                    print("    ğŸ”§ å°è¯•ä¿®å¤flake8é—®é¢˜...")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨ä¿®å¤é€»è¾‘
        
        # æ£€æŸ¥blackä»£ç æ ¼å¼
        if self._has_command('black'):
            black_result = self._run_command(['black', '--check', 'src/', 'tests/'])
            checks_performed.append('black')
            if black_result['returncode'] != 0:
                issues.append("Blackä»£ç æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                if self.auto_fix:
                    print("    ğŸ”§ è‡ªåŠ¨ä¿®å¤ä»£ç æ ¼å¼...")
                    fix_result = self._run_command(['black', 'src/', 'tests/'])
                    if fix_result['returncode'] == 0:
                        issues.remove("Blackä»£ç æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                        print("      âœ… ä»£ç æ ¼å¼å·²ä¿®å¤")
        
        # æ£€æŸ¥isortå¯¼å…¥æ’åº
        if self._has_command('isort'):
            isort_result = self._run_command(['isort', '--check-only', 'src/', 'tests/'])
            checks_performed.append('isort')
            if isort_result['returncode'] != 0:
                issues.append("Isortå¯¼å…¥æ’åºä¸ç¬¦åˆè¦æ±‚")
                if self.auto_fix:
                    print("    ğŸ”§ è‡ªåŠ¨ä¿®å¤å¯¼å…¥æ’åº...")
                    fix_result = self._run_command(['isort', 'src/', 'tests/'])
                    if fix_result['returncode'] == 0:
                        issues.remove("Isortå¯¼å…¥æ’åºä¸ç¬¦åˆè¦æ±‚")
                        print("      âœ… å¯¼å…¥æ’åºå·²ä¿®å¤")
        
        return {
            'checks_performed': checks_performed,
            'issues': issues,
            'issues_count': len(issues),
            'success': len(issues) == 0
        }
    
    def _generate_coverage_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
        cmd = [
            'python', '-m', 'pytest', 
            'tests/',
            '--cov=src',
            '--cov-report=term',
            '--cov-report=html',
            '--cov-report=json',
            '--cov-fail-under=50'
        ]
        
        if self.fast_mode:
            cmd.extend(['-m', 'not slow'])
        
        result = self._run_command(cmd)
        
        # è§£æè¦†ç›–ç‡æ•°æ®
        coverage_data = self._parse_coverage_data()
        
        return {
            'command': ' '.join(cmd),
            'returncode': result['returncode'],
            'coverage_data': coverage_data,
            'success': result['returncode'] == 0
        }
    
    def _run_command(self, cmd: List[str]) -> Dict[str, Any]:
        """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
        try:
            start_time = time.time()
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            duration = time.time() - start_time
            
            return {
                'returncode': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'duration': duration
            }
        except subprocess.TimeoutExpired:
            return {
                'returncode': -1,
                'stdout': '',
                'stderr': 'Command timeout after 300 seconds',
                'duration': 300
            }
        except Exception as e:
            return {
                'returncode': -1,
                'stdout': '',
                'stderr': str(e),
                'duration': 0
            }
    
    def _parse_pytest_output(self, output: str) -> Dict[str, int]:
        """è§£æpytestè¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'passed': 0,
            'failed': 0,
            'error': 0,
            'skipped': 0,
            'warnings': 0
        }
        
        lines = output.strip().split('\n')
        for line in reversed(lines):
            if any(keyword in line for keyword in ['passed', 'failed', 'error', 'skipped']):
                # è§£æç±»ä¼¼ "5 passed, 1 failed" çš„æ ¼å¼
                import re
                
                passed_match = re.search(r'(\d+)\s+passed', line)
                if passed_match:
                    stats['passed'] = int(passed_match.group(1))
                
                failed_match = re.search(r'(\d+)\s+failed', line)
                if failed_match:
                    stats['failed'] = int(failed_match.group(1))
                
                error_match = re.search(r'(\d+)\s+error', line)
                if error_match:
                    stats['error'] = int(error_match.group(1))
                
                skipped_match = re.search(r'(\d+)\s+skipped', line)
                if skipped_match:
                    stats['skipped'] = int(skipped_match.group(1))
                
                warnings_match = re.search(r'(\d+)\s+warning', line)
                if warnings_match:
                    stats['warnings'] = int(warnings_match.group(1))
                
                break
        
        return stats
    
    def _parse_coverage_data(self) -> Dict[str, Any]:
        """è§£æè¦†ç›–ç‡æ•°æ®"""
        coverage_json_file = self.project_root / 'coverage.json'
        if coverage_json_file.exists():
            try:
                with open(coverage_json_file, 'r') as f:
                    return json.load(f)
            except Exception:
                pass
        return {}
    
    def _calculate_summary(self):
        """è®¡ç®—æµ‹è¯•ç»Ÿè®¡æ‘˜è¦"""
        summary = self.validation_results['summary']
        
        for test_type, result in self.validation_results['test_results'].items():
            if 'stats' in result:
                stats = result['stats']
                summary['total_tests'] += stats['passed'] + stats['failed'] + stats['error']
                summary['passed_tests'] += stats['passed']
                summary['failed_tests'] += stats['failed']
                summary['error_tests'] += stats['error']
                summary['skipped_tests'] += stats['skipped']
        
        # è®¡ç®—æˆåŠŸç‡
        if summary['total_tests'] > 0:
            summary['success_rate'] = (summary['passed_tests'] / summary['total_tests']) * 100
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        # ç”ŸæˆJSONæŠ¥å‘Š
        json_report_file = self.reports_dir / f'test_validation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(json_report_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report_file = self.reports_dir / f'test_validation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html'
        self._generate_html_report(html_report_file)
        
        print(f"  ğŸ“„ JSONæŠ¥å‘Š: {json_report_file}")
        print(f"  ğŸ“Š HTMLæŠ¥å‘Š: {html_report_file}")
    
    def _generate_html_report(self, output_file: Path):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æµ‹è¯•éªŒè¯æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
        .warning {{ color: orange; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f2f2f2; }}
        .metric {{ display: inline-block; margin: 10px; padding: 10px; border: 1px solid #ccc; border-radius: 5px; text-align: center; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“ˆ æµ‹è¯•éªŒè¯æŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {self.validation_results['timestamp']}</p>
        <p><strong>é¡¹ç›®:</strong> {self.validation_results['project_info']['name']}</p>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
        <div class="metric">
            <h3>{self.validation_results['summary']['total_tests']}</h3>
            <p>æ€»æµ‹è¯•æ•°</p>
        </div>
        <div class="metric">
            <h3 class="success">{self.validation_results['summary']['passed_tests']}</h3>
            <p>é€šè¿‡</p>
        </div>
        <div class="metric">
            <h3 class="failure">{self.validation_results['summary']['failed_tests']}</h3>
            <p>å¤±è´¥</p>
        </div>
        <div class="metric">
            <h3>{self.validation_results['summary']['skipped_tests']}</h3>
            <p>è·³è¿‡</p>
        </div>
        <div class="metric">
            <h3>{self.validation_results['summary']['success_rate']:.1f}%</h3>
            <p>æˆåŠŸç‡</p>
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ“„ æµ‹è¯•ç»“æœè¯¦æƒ…</h2>
        <table>
            <tr>
                <th>æµ‹è¯•ç±»å‹</th>
                <th>çŠ¶æ€</th>
                <th>é€šè¿‡</th>
                <th>å¤±è´¥</th>
                <th>è€—æ—¶(ç§’)</th>
            </tr>
"""
        
        for test_type, result in self.validation_results['test_results'].items():
            status = "âœ… æˆåŠŸ" if result.get('success', False) else "âŒ å¤±è´¥"
            stats = result.get('stats', {})
            duration = result.get('duration', 0)
            
            html_content += f"""
            <tr>
                <td>{test_type}</td>
                <td>{status}</td>
                <td>{stats.get('passed', 0)}</td>
                <td>{stats.get('failed', 0)}</td>
                <td>{duration:.2f}</td>
            </tr>
"""
        
        html_content += """
        </table>
    </div>
    
    <div class="section">
        <h2>âš ï¸ é—®é¢˜å’Œå»ºè®®</h2>
"""
        
        if self.validation_results['issues']:
            html_content += "<h3>ğŸš¨ å‘ç°çš„é—®é¢˜:</h3><ul>"
            for issue in self.validation_results['issues']:
                html_content += f"<li class='failure'>{issue}</li>"
            html_content += "</ul>"
        
        if self.validation_results['recommendations']:
            html_content += "<h3>ğŸ’¡ å»ºè®®:</h3><ul>"
            for rec in self.validation_results['recommendations']:
                html_content += f"<li>{rec}</li>"
            html_content += "</ul>"
        
        html_content += """
    </div>
</body>
</html>
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _display_summary(self, success: bool):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦"""
        summary = self.validation_results['summary']
        
        print("\n" + "="*60)
        print("ğŸ“ˆ æµ‹è¯•éªŒè¯ç»“æœæ‘˜è¦")
        print("="*60)
        
        status_icon = "âœ…" if success else "âŒ"
        status_text = "æ‰€æœ‰æµ‹è¯•é€šè¿‡" if success else "æœ‰æµ‹è¯•å¤±è´¥"
        print(f"{status_icon} æ€»ä½“çŠ¶æ€: {status_text}")
        
        print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"  é€šè¿‡: {summary['passed_tests']} âœ…")
        print(f"  å¤±è´¥: {summary['failed_tests']} âŒ")
        print(f"  é”™è¯¯: {summary['error_tests']} âš ï¸")
        print(f"  è·³è¿‡: {summary['skipped_tests']} â­ï¸")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print(f"  æ‰§è¡Œæ—¶é—´: {summary['execution_time']:.2f}ç§’")
        
        if self.validation_results['issues']:
            print(f"\nâš ï¸ å‘ç° {len(self.validation_results['issues'])} ä¸ªé—®é¢˜:")
            for i, issue in enumerate(self.validation_results['issues'][:5], 1):
                print(f"  {i}. {issue}")
            if len(self.validation_results['issues']) > 5:
                print(f"  ... è¿˜æœ‰ {len(self.validation_results['issues']) - 5} ä¸ªé—®é¢˜")
        
        print("\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ tests/reports/ ç›®å½•")
    
    def _get_project_info(self) -> Dict[str, Any]:
        """è·å–é¡¹ç›®ä¿¡æ¯"""
        try:
            import toml
            pyproject_file = self.project_root / 'pyproject.toml'
            if pyproject_file.exists():
                with open(pyproject_file, 'r') as f:
                    pyproject_data = toml.load(f)
                    return pyproject_data.get('tool', {}).get('poetry', {})
        except ImportError:
            pass
        
        return {
            'name': 'AShare Agent',
            'version': '0.1.0',
            'description': 'AI-powered hedge fund system'
        }
    
    def _has_command(self, command: str) -> bool:
        """æ£€æŸ¥å‘½ä»¤æ˜¯å¦å­˜åœ¨"""
        result = self._run_command([command, '--version'])
        return result['returncode'] == 0


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æµ‹è¯•éªŒè¯è„šæœ¬')
    parser.add_argument('--fast', action='store_true', help='å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡æ…¢é€Ÿæµ‹è¯•')
    parser.add_argument('--coverage', action='store_true', help='ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š')
    parser.add_argument('--no-report', action='store_true', help='ä¸ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--fix', action='store_true', help='è‡ªåŠ¨ä¿®å¤å¯ä¿®å¤çš„é—®é¢˜')
    
    args = parser.parse_args()
    
    validator = TestValidator(
        fast_mode=args.fast,
        with_coverage=args.coverage,
        generate_report=not args.no_report,
        auto_fix=args.fix
    )
    
    success = validator.run_validation()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
